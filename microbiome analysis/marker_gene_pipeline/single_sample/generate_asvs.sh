#!/usr/bin/env bash
set -eo pipefail

# Authors:
#   - Sarah Nadeau (sarah@primediscoveries.com)
#   - Arun Manoharan (arun@primediscoveries.com)
# Created by Sarah Nadeau
# Last Updated February 2019 by Arun Manoharan

# This script generates an ASV feature table and representative sequences from a QC'd sample.
#
# The script will create its output in a directory specific to the denoiser,
# trim length, and read type (single vs paired) applied. This allows the script
# to be rerun with different denoising parameters and supports downstream
# merging of ASV feature tables generated from compatible parameter sets.
#
# The analyses performed in this script are based on qiita/deblur recommendations for performing meta-analyses (i.e. combining studies).
# There isn't a comprehensive or authoritative list of recommendations, and meta-analysis "best practices" are always evolving.
# The following resources were used to determine the current best practices for meta-analysis:
#
# - https://qiita.ucsd.edu/static/doc/html/processingdata/processing-recommendations.html
# - https://qiita.ucsd.edu/static/doc/html/processingdata/deblur_quality.html
# - https://cmi-workshop.readthedocs.io/en/latest/qiita-16S-analysis.html#analysis-of-deblur-processed-data
# - https://forum.qiime2.org/t/transferring-qiita-artifacts-to-qiime2/4790
# - https://forum.qiime2.org/t/running-deblur-on-paired-end-sequences/3387
# - https://forum.qiime2.org/t/combining-datasets-with-2-sets-of-primers/3073/12
# - https://genome.cshlp.org/content/23/10/1704.long

# TODO
#
# - Script currently denoises single-end QC'd sequences produced by quality_control.sh
#   (if data are paired-end, forward reads are used). Support should be added in the future to denoise
#   paired-end data (dada2 will merge the sequences during denoising; deblur requires the sequences to
#   be merged beforehand).
#
# - Support other reference databases in addition to the default gg_13_8 88% OTUs when denoising with deblur (used in deblur's initial reference-based filtering).

# Requirements:
#
# - SAMPLE_DIR must contain the output generated by quality_control.sh.
#
# - The sequences must be 16S data.
#
# - The sequences processed by this pipeline must be Illumina sequences; other sequencing platforms (e.g. 454 data) are not currently supported.

# Program requirements:
#     qiime2-2019.4 conda environment (use install/create_conda_envs.sh to create one)

# Script options:
#     --sample_dir SAMPLE_DIR: path to sample directory [REQUIRED]
#     --denoiser DENOISER: denoising method to create ASV feature table. Options are 'deblur' and 'dada2' [REQUIRED]
#     --trim_len TRIM_LEN: uniform sequence trim length for denoising [REQUIRED]
#     --min_read_count MIN_READ_COUNT: minimum number of reads that must be present in the sample after denoising [default: 5000]
#     --bloom_seqs BLOOM_SEQS: path to fasta file of bloom sequences to filter from ASV table and representative sequences. A representative sequence is identified as bloom if it is an exact match to a bloom sequence, or if one is a prefix of the other (i.e. if representative sequence and bloom sequence lengths differ) [default: no bloom filtering]
#     --threads THREADS: number of threads to use when denoising with dada2. Note: this option is not used when denoising with deblur [default: 8]
#     --qiime2_env QIIME2_ENV: qiime2 conda environment name [default: 'qiime2-2019.4']

# Outputs:
#     ${SAMPLE_DIR}/asvs/${DENOISER}_single_end_trim_len_${TRIM_LEN}/
#         generate_asvs.log: log of commands started and completed during running of this script
#         denoised: directory containing ASV feature table and representative sequences

# Example usage:
#     bash generate_asvs.sh
#         --sample_dir /path/to/sample/directory
#         --denoiser deblur
#         --trim_len 150

SAMPLE_DIR=""
DENOISER=""
TRIM_LEN=""
MIN_READ_COUNT="5000"
BLOOM_SEQS=""
THREADS="8"
QIIME2_ENV="qiime2-2019.4"

# https://stackoverflow.com/a/14203146/3776794
while [[ $# -gt 0 ]]; do
    key="${1}"

    case "${key}" in
        --sample_dir)
            SAMPLE_DIR="${2}"
            shift 2
            ;;
        --denoiser)
            DENOISER="${2}"
            shift 2
            ;;
        --trim_len)
            TRIM_LEN="${2}"
            shift 2
            ;;
        --min_read_count)
            MIN_READ_COUNT="${2}"
            shift 2
            ;;
        --bloom_seqs)
            BLOOM_SEQS="${2}"
            shift 2
            ;;
        --threads)
            THREADS="${2}"
            shift 2
            ;;
        --qiime2_env)
            QIIME2_ENV="${2}"
            shift 2
            ;;
        *)
            echo "Error: Unknown option '${key}'. Please see script header for usage instructions."
            exit 1
            ;;
    esac
done

if [ -z "${SAMPLE_DIR}" ]; then
    echo "Error: --sample_dir is a required option."
    exit 1
fi

if [ -z "${DENOISER}" ]; then
    echo "Error: --denoiser is a required option."
    exit 1
fi

case "${DENOISER}" in
    deblur|dada2)
        ;;
    *)
        echo "Error: Unrecognized denoising method: '${DENOISER}'. Valid choices are 'deblur' or 'dada2'."
        exit 1
        ;;
esac

if [ -z "${TRIM_LEN}" ]; then
    echo "Error: --trim_len is a required option."
    exit 1
fi

OUTPUT_DIR="${SAMPLE_DIR}/asvs/${DENOISER}_single_end_trim_len_${TRIM_LEN}"

if [ -d "${OUTPUT_DIR}" ]; then
  echo "Error: ${OUTPUT_DIR} directory already exists. Please either remove the directory and rerun this script, specify a different --sample_dir, or provide a different set of denoiser parameters."
  exit 1
fi

# loading common functions
# spellcheck source=../../lib/util/helpers.sh
. "$(dirname ${BASH_SOURCE[0]})/../../lib/util/helpers.sh"

mkdir -p ${OUTPUT_DIR}

# Writes message with timestamp to log file and also displays on stdout. Will
# create log file if it doesn't already exist.
function log_message {
    set -eo pipefail

    MSG="${1}"
    SCRIPT_LOG="${OUTPUT_DIR}/generate_asvs.log"

    if [ ! -f "${SCRIPT_LOG}" ]; then
        echo -e "generate_asvs.sh log file\n" > "${SCRIPT_LOG}"
    fi

    echo "$(date +%Y-%m-%d:%H:%M:%S) ${MSG}" | tee -a "${SCRIPT_LOG}"
}

# Save pipeline start timestamp for writing to pipeline_completed.log file at
# end of script.
PIPELINE_STARTED="$(date +%Y-%m-%d:%H:%M:%S)"
log_message "Pipeline execution started."

log_message "Validating quality_control directory started."

QC_DIR="${SAMPLE_DIR}/quality_control"

if [ ! -d "${QC_DIR}" ]; then
  echo "Error: ${QC_DIR} directory doesn't exist. Please run quality_control.sh prior to running this script."
  exit 1
fi

TARGET_GENE="$(grep TARGET_GENE ${QC_DIR}/qc_params.log | cut -f2 -d ' ')"
if [[ "${TARGET_GENE}" != "16S" ]]; then
    echo "Error: '${TARGET_GENE}' is not a supported target gene. The only supported target gene at this time is '16S'."
    exit 1
fi

log_message "Validating quality_control directory completed."

# Create directories for output
log_message "Creating output directories started."

mkdir -p ${OUTPUT_DIR}/denoised

log_message "Creating output directories completed."

# export a copy of input parameters
log_message "Writing file of input parameters started."
PARAMS_LOG="${OUTPUT_DIR}/generate_asvs_params.log"

# Compute MD5 sum of the bloom seqs fasta file if one was provided. This
# information is used by merge_samples.py to determine if the same bloom filter
# was used for each sample (samples should only be merged if the same bloom
# filter was applied).
if [ -z "${BLOOM_SEQS}" ]; then
    BLOOM_SEQS_MD5=""
else
    # Note: this isn't particularly efficient because the entire bloom seqs
    # file is read into memory. The bloom seqs file is expected to always be
    # small so this shouldn't be an issue in practice (e.g. American Gut bloom
    # seqs file is ~4KB). If performance becomes an issue, it's straightforward
    # to chunk the file (but makes for a gross one-liner).
    #
    # Note: using a Python one-liner to compute MD5 for cross-platform
    # compatibility.
    BLOOM_SEQS_MD5="$(python -c "import hashlib; print(hashlib.md5(open('${BLOOM_SEQS}', 'rb').read()).hexdigest());")"
fi

echo "SAMPLE_DIR: $SAMPLE_DIR" > "${PARAMS_LOG}"
echo "DENOISER: $DENOISER" >> "${PARAMS_LOG}"
echo "TRIM_LEN: $TRIM_LEN" >> "${PARAMS_LOG}"
echo "MIN_READ_COUNT: $MIN_READ_COUNT" >> "${PARAMS_LOG}"
echo "BLOOM_SEQS: $BLOOM_SEQS" >> "${PARAMS_LOG}"
echo "BLOOM_SEQS_MD5: $BLOOM_SEQS_MD5" >> "${PARAMS_LOG}"
echo "THREADS: $THREADS" >> "${PARAMS_LOG}"
echo "QIIME2_ENV: $QIIME2_ENV" >> "${PARAMS_LOG}"

log_message "Writing file of input parameters completed."

source activate ${QIIME2_ENV}

log_message "Denoising sequences and generating ASV feature table with ${DENOISER} started."

DEMUX_SEQS_PATH="${QC_DIR}/single_end_qc/qc_seqs.qza"

if [[ "${DENOISER}" == "deblur" ]]; then
    # Generate feature table (ASVs) using deblur in qiime2.
    #
    # Deblur will automatically filter out PhiX and adapter-contaminated reads, as
    # well as non-16S reads (by matching against Greengenes 13_8 88% OTUs).
    #
    # Note: setting --p-min-reads and --p-min-size based on these recommendations:
    #
    # - https://forum.qiime2.org/t/transferring-qiita-artifacts-to-qiime2/4790
    # - https://github.com/qiita-spots/qp-deblur/blob/d8e74a143302f60231f6a4dfcd6328d8a94f131c/qp_deblur/__init__.py#L65-L70
    #
    # Not using `--p-jobs-to-start ${THREADS}` because deblur uses a separate
    # job per sample, but we're only processing a single sample here.
    # `deblur workflow` supports a `--threads-per-sample` option that could
    # speed things up, but that option isn't available in the qiime2 deblur
    # plugin wrapper. If the option becomes available in the future, we should
    # use it, or consider using `deblur workflow` directly instead of the
    # qiime2 plugin.
    #
    # TODO look into using `qiime deblur denoise-other` with `--i-reference-seqs` to specify a database other than the default greengenes 88% OTUs.
    if ! qiime deblur denoise-16S \
        --i-demultiplexed-seqs ${DEMUX_SEQS_PATH} \
        --p-trim-length ${TRIM_LEN} \
        --p-min-reads 0 \
        --p-min-size 2 \
        --p-sample-stats \
        --o-table ${OUTPUT_DIR}/denoised/feature_table.qza \
        --o-representative-sequences ${OUTPUT_DIR}/denoised/rep_seqs.qza \
        --o-stats ${OUTPUT_DIR}/denoised/deblur_stats.qza \
        --verbose \
        2>&1 | tee -a ${OUTPUT_DIR}/q2_deblur.log; then
            notify_value_error_from "${OUTPUT_DIR}/q2_deblur.log"
            exit 1
    fi
elif [[ "${DENOISER}" == "dada2" ]]; then
    # Generate feature table (ASVs) using dada2 in qiime2.
    #
    # dada2 will automatically filter out PhiX reads. dada2 expects adapters
    # and any other non-biological sequence artifacts (e.g. primers, barcodes)
    # to have previously been trimmed from sequences. dada2 does not perform
    # any reference-based filtering like deblur does.
    if ! qiime dada2 denoise-single \
        --i-demultiplexed-seqs ${DEMUX_SEQS_PATH} \
        --p-trunc-len ${TRIM_LEN} \
        --p-n-threads ${THREADS} \
        --o-table ${OUTPUT_DIR}/denoised/feature_table.qza \
        --o-representative-sequences ${OUTPUT_DIR}/denoised/rep_seqs.qza \
        --o-denoising-stats ${OUTPUT_DIR}/denoised/dada2_stats.qza \
        --verbose \
        2>&1 | tee -a ${OUTPUT_DIR}/q2_dada2.log; then
            notify_value_error_from "${OUTPUT_DIR}/q2_dada2.log"
            exit 1
    fi
fi

log_message "Denoising sequences and generating ASV feature table with ${DENOISER} completed."

# Find directory this script is contained in:
# https://stackoverflow.com/a/246128/3776794
REPO_ROOT_DIR="$(dirname "$(dirname "$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)")")"

if [ -z "${BLOOM_SEQS}" ]; then
    log_message "Skipping bloom filter because bloom sequences weren't provided via --bloom_seqs option."
else
    log_message "Filtering bloom sequences from ASV feature table and representative sequences started."

    python "${REPO_ROOT_DIR}/lib/util/filter_blooms.py" \
        --feature_table "${OUTPUT_DIR}/denoised/feature_table.qza" \
        --rep_seqs "${OUTPUT_DIR}/denoised/rep_seqs.qza" \
        --bloom_seqs "${BLOOM_SEQS}" \
        --output_dir "${OUTPUT_DIR}/denoised/bloom_filtered"

    mv "${OUTPUT_DIR}/denoised/bloom_filtered/feature_table.qza" "${OUTPUT_DIR}/denoised/feature_table.qza"
    mv "${OUTPUT_DIR}/denoised/bloom_filtered/rep_seqs.qza" "${OUTPUT_DIR}/denoised/rep_seqs.qza"
    mv "${OUTPUT_DIR}/denoised/bloom_filtered/stats.tsv" "${OUTPUT_DIR}/denoised/filter_blooms_stats.tsv"
    rmdir "${OUTPUT_DIR}/denoised/bloom_filtered"

    log_message "Filtering bloom sequences from ASV feature table and representative sequences completed."
fi

log_message "Exporting ASV feature table and representative sequences in fasta format (for kmer pipeline) started."

python "${REPO_ROOT_DIR}/lib/util/feature_table_to_fasta.py" \
    --feature_table "${OUTPUT_DIR}/denoised/feature_table.qza" \
    --rep_seqs "${OUTPUT_DIR}/denoised/rep_seqs.qza" \
    --output_dir "${OUTPUT_DIR}/denoised/fasta"

log_message "Exporting ASV feature table and representative sequences in fasta format (for kmer pipeline) completed."

log_message "Checking sample has at least ${MIN_READ_COUNT} denoised reads started."

# Sum the number of reads in the table (we can sum the entire table because it
# only contains a single sample). We know the counts will be integers, so
# convert to an integer before doing bash integer comparisons below
# (`biom.Table.sum()` outputs a numpy float).
READ_COUNT="$(python -c "import qiime2; import biom; print(int(qiime2.Artifact.load('${OUTPUT_DIR}/denoised/feature_table.qza').view(biom.Table).sum()))")"

if [[ "${READ_COUNT}" -lt "${MIN_READ_COUNT}" ]]; then
    MSG="Error: Sample has ${READ_COUNT} denoised reads, which is less than the required minimum read count of ${MIN_READ_COUNT}. This sample may not have enough denoised reads for downstream analyses, and further investigation into why the sample has a low read count is recommended. You may specify a different --min_read_count to change the required minimum read count."
    log_message "${MSG}"
    notify_app "${MSG}" "HARD_ERROR"
    exit 1
else
    log_message "Sample has ${READ_COUNT} denoised reads."
fi

log_message "Checking sample has at least ${MIN_READ_COUNT} denoised reads completed."

# Deactivate qiime2 conda env.
conda deactivate

PIPELINE_COMPLETED="$(date +%Y-%m-%d:%H:%M:%S)"

echo "${PIPELINE_STARTED} Pipeline execution started." > "${OUTPUT_DIR}/pipeline_completed.log"
echo "${PIPELINE_COMPLETED} Pipeline execution completed." >> "${OUTPUT_DIR}/pipeline_completed.log"
log_message "Pipeline execution completed."
